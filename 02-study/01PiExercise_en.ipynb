{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Academic exercise for study"
      ],
      "metadata": {
        "id": "pPSozPKFYNfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install pyspark and customize Colab configuration"
      ],
      "metadata": {
        "id": "DICbEoexYi57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python interface to Spark\n",
        "!pip install pyspark --quiet\n",
        "# Installation and update of the PyDrive library, for interacting with Google Drive using Python.\n",
        "!pip install -U -q PyDrive --quiet\n",
        "# Install OpenJDK 8\n",
        "!apt install openjdk-8-jdk-headless &> /dev/null\n",
        "# Download the ngrok zip file to access the local server over the internet\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Unzip the ngrok zip file\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Starts ngrok, allowing HTTP traffic on port 4050\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "# Import the Python os module\n",
        "import os\n",
        "# Sets the JAVA_HOME environment variable to the location of Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tGeNmsAYiaP",
        "outputId": "b3c4d1cf-201e-4917-ec19-eb997f2f3012"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Spark"
      ],
      "metadata": {
        "id": "OrafHEfsYtRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from random import random\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"pi\").setMaster(\"local[2]\")\n",
        "sc = SparkSession.builder.config(conf=conf).getOrCreate()"
      ],
      "metadata": {
        "id": "xIgknxGnPEIG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The code demonstrates how to use PySpark to perform distributed computations and estimate the value of pi using the Monte Carlo method**"
      ],
      "metadata": {
        "id": "QYW5aDOgdSxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def sample(p):\n",
        "This is a function definition for the sample function.\n",
        "The function takes a single parameter p.\n",
        "Inside the function, it generates random values for x and y.\n",
        "It returns 1 if the point (x, y) falls within the unit circle (x^2 + y^2 < 1), otherwise it returns 0.\n",
        "\"\"\"\n",
        "def sample(p):\n",
        "    x,y = random(), random()\n",
        "    return 1 if x*x + y*y < 1 else 0\n",
        "# This line defines the number of samples (points) to be generated for the Monte Carlo estimation of pi.\n",
        "NUM_SAMPLES = 1000*1000*100\n",
        "\"\"\"\n",
        "This line parallelizes a range of numbers from 0 to NUM_SAMPLES using Spark's parallelize method.\n",
        "It distributes the numbers across two partitions for parallel processing.\n",
        "The resulting RDD (Resilient Distributed Dataset) is assigned to the variable items.\n",
        "\"\"\"\n",
        "items = sc.sparkContext.parallelize(range(0, NUM_SAMPLES), 50)\n",
        "\"\"\"\n",
        "This line applies the sample function to each item in the items RDD using the map transformation.\n",
        "The resulting RDD contains 1s and 0s indicating whether each point falls within the unit circle.\n",
        "The reduce action is then used to sum up all the 1s and 0s to get the total count.\n",
        "The final count is assigned to the variable count.\n",
        "\"\"\"\n",
        "count = items.map(sample).reduce(lambda a, b: a + b)\n",
        "# Print the result\n",
        "# This line prints the estimated value of pi by dividing the count of points inside the unit circle by the total number of points and multiplying by 4.\n",
        "print(\"Pi is roughly %.9f\" % (4.0 * count / NUM_SAMPLES))\n",
        "# This line prints the default parallelism level, which indicates the number of partitions used by default when parallelizing data.\n",
        "print(\"Default parallelism: {}\".format(sc.sparkContext.defaultParallelism))\n",
        "# This line prints the number of partitions in the items RDD.\n",
        "print(\"Number of partitions: {}\".format(items.getNumPartitions()))\n",
        "# This line prints the partitioner used by the items RDD, which determines how the data is distributed across partitions.\n",
        "print(\"Partitioner: {}\".format(items.partitioner))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXii4-MTPEFg",
        "outputId": "b8920d13-0970-42fe-f517-c6e3f3c72f22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi is roughly 3.139906000\n",
            "Default parallelism: 2\n",
            "Number of partitions: 50\n",
            "Partitioner: None\n"
          ]
        }
      ]
    }
  ]
}