{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Academic exercise for study\n"
      ],
      "metadata": {
        "id": "lidiaC7WWbKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment installation and configuration"
      ],
      "metadata": {
        "id": "GcpdU1pKZ5_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X76WV3AJp1v",
        "outputId": "9f9f873a-87b5-4231-9a7a-0778c835cdd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Python interface to Spark\n",
        "!pip install pyspark --quiet\n",
        "# Installation and update of the PyDrive library, for interacting with Google Drive using Python.\n",
        "!pip install -U -q PyDrive --quiet\n",
        "# Install OpenJDK 8\n",
        "!apt install openjdk-8-jdk-headless &> /dev/null\n",
        "# Download the ngrok zip file to access the local server over the internet\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Unzip the ngrok zip file\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Starts ngrok, allowing HTTP traffic on port 4050\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "# Import the Python os module\n",
        "import os\n",
        "# Sets the JAVA_HOME environment variable to the location of Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up and starting a Spark session using the PySpark library"
      ],
      "metadata": {
        "id": "56AF03Q7Z-PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"housing\").setMaster(\"local[2]\")\n",
        "sc = SparkSession.builder.config(conf=conf).getOrCreate()"
      ],
      "metadata": {
        "id": "c9nIrvhRJzim"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "AOP_H-rTaW9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "# Path\n",
        "url = 'https://raw.githubusercontent.com/E-man85/Big-Data/main/03-data/boston.csv'\n",
        "# Local path to save the file\n",
        "local_file_path = '/content/boston.csv'\n",
        "# Download file from remote URL\n",
        "urllib.request.urlretrieve(url, local_file_path)\n",
        "df_spark = sc.read.csv(\"/content/boston.csv\", inferSchema=True, header=True)\n",
        "# View schema\n",
        "df_spark.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9QBR8kfJ1G9",
        "outputId": "20308eaa-9d8a-414b-b0bd-c57d75632e9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRIM: double (nullable = true)\n",
            " |-- ZN: double (nullable = true)\n",
            " |-- INDUS: double (nullable = true)\n",
            " |-- CHAS: integer (nullable = true)\n",
            " |-- NOX: double (nullable = true)\n",
            " |-- RM: double (nullable = true)\n",
            " |-- AGE: double (nullable = true)\n",
            " |-- DIS: double (nullable = true)\n",
            " |-- RAD: integer (nullable = true)\n",
            " |-- TAX: double (nullable = true)\n",
            " |-- PTRATIO: double (nullable = true)\n",
            " |-- B: double (nullable = true)\n",
            " |-- LSTAT: double (nullable = true)\n",
            " |-- MEDV: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark SQL operations"
      ],
      "metadata": {
        "id": "CskHAVFCakDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a temporary view\n",
        "df_spark.createOrReplaceTempView(\"data_view\")\n",
        "# Perform Spark SQL operations\n",
        "result = sc.sql(\"SELECT CRIM, INDUS, MEDV FROM data_view WHERE CRIM > 5.0\")\n",
        "# Show the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msrrheSjLr7K",
        "outputId": "2f6b2f15-2200-4049-a836-6fb5053ee670"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+\n",
            "|   CRIM|INDUS|MEDV|\n",
            "+-------+-----+----+\n",
            "|8.98296| 18.1|17.8|\n",
            "|5.20177| 18.1|22.7|\n",
            "|13.5222| 18.1|23.1|\n",
            "|5.66998| 18.1|50.0|\n",
            "|6.53876| 18.1|50.0|\n",
            "| 9.2323| 18.1|50.0|\n",
            "|8.26725| 18.1|50.0|\n",
            "|11.1081| 18.1|13.8|\n",
            "|18.4982| 18.1|13.8|\n",
            "|19.6091| 18.1|15.0|\n",
            "| 15.288| 18.1|13.9|\n",
            "|9.82349| 18.1|13.3|\n",
            "|23.6482| 18.1|13.1|\n",
            "|17.8667| 18.1|10.2|\n",
            "|88.9762| 18.1|10.4|\n",
            "|15.8744| 18.1|10.9|\n",
            "|9.18702| 18.1|11.3|\n",
            "|7.99248| 18.1|12.3|\n",
            "|20.0849| 18.1| 8.8|\n",
            "|16.8118| 18.1| 7.2|\n",
            "+-------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD operations"
      ],
      "metadata": {
        "id": "bq8tq0uLamwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform RDD operations\n",
        "rdd_crim = df_spark.rdd\n",
        "result_rdd = rdd_crim.filter(lambda row: row[0] > 5.0) \\\n",
        "    .map(lambda row: (row[0], row[2], row[13]))\n",
        "# Show the result\n",
        "for row in result_rdd.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0COvzCi4MX8D",
        "outputId": "b4390029-09f7-4465-dec3-74ab45fa83f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8.98296, 18.1, 17.8)\n",
            "(5.20177, 18.1, 22.7)\n",
            "(13.5222, 18.1, 23.1)\n",
            "(5.66998, 18.1, 50.0)\n",
            "(6.53876, 18.1, 50.0)\n",
            "(9.2323, 18.1, 50.0)\n",
            "(8.26725, 18.1, 50.0)\n",
            "(11.1081, 18.1, 13.8)\n",
            "(18.4982, 18.1, 13.8)\n",
            "(19.6091, 18.1, 15.0)\n",
            "(15.288, 18.1, 13.9)\n",
            "(9.82349, 18.1, 13.3)\n",
            "(23.6482, 18.1, 13.1)\n",
            "(17.8667, 18.1, 10.2)\n",
            "(88.9762, 18.1, 10.4)\n",
            "(15.8744, 18.1, 10.9)\n",
            "(9.18702, 18.1, 11.3)\n",
            "(7.99248, 18.1, 12.3)\n",
            "(20.0849, 18.1, 8.8)\n",
            "(16.8118, 18.1, 7.2)\n",
            "(24.3938, 18.1, 10.5)\n",
            "(22.5971, 18.1, 7.4)\n",
            "(14.3337, 18.1, 10.2)\n",
            "(8.15174, 18.1, 11.5)\n",
            "(6.96215, 18.1, 15.1)\n",
            "(5.29305, 18.1, 23.2)\n",
            "(11.5779, 18.1, 9.7)\n",
            "(8.64476, 18.1, 13.8)\n",
            "(13.3598, 18.1, 12.7)\n",
            "(8.71675, 18.1, 13.1)\n",
            "(5.87205, 18.1, 12.5)\n",
            "(7.67202, 18.1, 8.5)\n",
            "(38.3518, 18.1, 5.0)\n",
            "(9.91655, 18.1, 6.3)\n",
            "(25.0461, 18.1, 5.6)\n",
            "(14.2362, 18.1, 7.2)\n",
            "(9.59571, 18.1, 12.1)\n",
            "(24.8017, 18.1, 8.3)\n",
            "(41.5292, 18.1, 8.5)\n",
            "(67.9208, 18.1, 5.0)\n",
            "(20.7162, 18.1, 11.9)\n",
            "(11.9511, 18.1, 27.9)\n",
            "(7.40389, 18.1, 17.2)\n",
            "(14.4383, 18.1, 27.5)\n",
            "(51.1358, 18.1, 15.0)\n",
            "(14.0507, 18.1, 17.2)\n",
            "(18.811, 18.1, 17.9)\n",
            "(28.6558, 18.1, 16.3)\n",
            "(45.7461, 18.1, 7.0)\n",
            "(18.0846, 18.1, 7.2)\n",
            "(10.8342, 18.1, 7.5)\n",
            "(25.9406, 18.1, 10.4)\n",
            "(73.5341, 18.1, 8.8)\n",
            "(11.8123, 18.1, 8.4)\n",
            "(11.0874, 18.1, 16.7)\n",
            "(7.02259, 18.1, 14.2)\n",
            "(12.0482, 18.1, 20.8)\n",
            "(7.05042, 18.1, 13.4)\n",
            "(8.79212, 18.1, 11.7)\n",
            "(15.8603, 18.1, 8.3)\n",
            "(12.2472, 18.1, 10.2)\n",
            "(37.6619, 18.1, 10.9)\n",
            "(7.36711, 18.1, 11.0)\n",
            "(9.33889, 18.1, 9.5)\n",
            "(8.49213, 18.1, 14.5)\n",
            "(10.0623, 18.1, 14.1)\n",
            "(6.44405, 18.1, 16.1)\n",
            "(5.58107, 18.1, 14.3)\n",
            "(13.9134, 18.1, 11.7)\n",
            "(11.1604, 18.1, 13.4)\n",
            "(14.4208, 18.1, 9.6)\n",
            "(15.1772, 18.1, 8.7)\n",
            "(13.6781, 18.1, 8.4)\n",
            "(9.39063, 18.1, 12.8)\n",
            "(22.0511, 18.1, 10.5)\n",
            "(9.72418, 18.1, 17.1)\n",
            "(5.66637, 18.1, 18.4)\n",
            "(9.96654, 18.1, 15.4)\n",
            "(12.8023, 18.1, 10.8)\n",
            "(10.6718, 18.1, 11.8)\n",
            "(6.28807, 18.1, 14.9)\n",
            "(9.92485, 18.1, 12.6)\n",
            "(9.32909, 18.1, 14.1)\n",
            "(7.52601, 18.1, 13.0)\n",
            "(6.71772, 18.1, 13.4)\n",
            "(5.44114, 18.1, 15.2)\n",
            "(5.09017, 18.1, 16.1)\n",
            "(8.24809, 18.1, 17.8)\n",
            "(9.51363, 18.1, 14.9)\n",
            "(8.20058, 18.1, 13.5)\n",
            "(7.75223, 18.1, 14.9)\n",
            "(6.80117, 18.1, 20.0)\n",
            "(6.65492, 18.1, 19.5)\n",
            "(5.82115, 18.1, 20.2)\n",
            "(7.83932, 18.1, 21.4)\n",
            "(15.5757, 18.1, 19.1)\n",
            "(13.0751, 18.1, 20.1)\n",
            "(8.05579, 18.1, 13.8)\n",
            "(6.39312, 18.1, 13.3)\n",
            "(15.0234, 18.1, 12.0)\n",
            "(10.233, 18.1, 14.6)\n",
            "(14.3337, 18.1, 21.4)\n",
            "(5.82401, 18.1, 23.0)\n",
            "(5.70818, 18.1, 23.7)\n",
            "(5.73116, 18.1, 25.0)\n",
            "(5.69175, 18.1, 19.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate descriptive statistics"
      ],
      "metadata": {
        "id": "fAo_VWvabtgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.mllib.stat import Statistics\n",
        "#rdd_stats = df_spark.rdd\n",
        "#summary = Statistics.colStats(rdd_stats)\n",
        "## Print the result\n",
        "#print(\"Count:\", summary.count())\n",
        "#print(\"Mean:\", summary.mean())\n",
        "#print(\"Variance:\", summary.variance())\n",
        "#print(\"Non-zero values:\", summary.numNonzeros())\n",
        "#print(\"Maximum:\", summary.max())\n",
        "#print(\"Minimum:\", summary.min())"
      ],
      "metadata": {
        "id": "z-7iwJXMNs_b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate descriptive statistics\n",
        "statistics_df = df_spark.describe()\n",
        "# Show the result\n",
        "statistics_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar88FClnOKyZ",
        "outputId": "53939292-f368-4d6a-93b5-0d96e66ce49a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|              CRIM|                ZN|             INDUS|              CHAS|                NOX|                RM|               AGE|              DIS|              RAD|               TAX|           PTRATIO|                 B|             LSTAT|              MEDV|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|               506|               506|               506|               506|                506|               506|               506|              506|              506|               506|               506|               506|               506|               506|\n",
            "|   mean|3.6135235573122535|11.363636363636363|11.136778656126504|0.0691699604743083| 0.5546950592885372| 6.284634387351787| 68.57490118577078|3.795042687747034|9.549407114624506| 408.2371541501976|18.455533596837967|356.67403162055257|12.653063241106723|22.532806324110698|\n",
            "| stddev| 8.601545105332491| 23.32245299451514| 6.860352940897589|0.2539940413404101|0.11587767566755584|0.7026171434153232|28.148861406903595| 2.10571012662761|8.707259384239366|168.53711605495903|2.1649455237144455| 91.29486438415782| 7.141061511348571| 9.197104087379815|\n",
            "|    min|           0.00632|               0.0|              0.46|                 0|              0.385|             3.561|               2.9|           1.1296|                1|             187.0|              12.6|              0.32|              1.73|               5.0|\n",
            "|    max|           88.9762|             100.0|             27.74|                 1|              0.871|              8.78|             100.0|          12.1265|               24|             711.0|              22.0|             396.9|             37.97|              50.0|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the data for training"
      ],
      "metadata": {
        "id": "F_2zVnQJcU9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Creating a VectorAssembler object\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "data_features = assembler.transform(df_spark).select(\"features\", \"MEDV\")\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = data_features.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "e9bnhelvND_E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression model"
      ],
      "metadata": {
        "id": "M3uGANvLdhtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(labelCol=\"MEDV\")\n",
        "# Train the model\n",
        "model = lr.fit(train_data)\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "# Display predicted and actual prices\n",
        "predictions.select(\"prediction\", \"MEDV\").show()\n",
        "# Evaluate the model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"MEDV\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWsjIatbcxn3",
        "outputId": "2f69b410-f17f-4024-ba1e-508182addd33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+\n",
            "|        prediction|MEDV|\n",
            "+------------------+----+\n",
            "| 27.48227401818597|22.0|\n",
            "| 40.59821928572499|50.0|\n",
            "|31.560171030407147|29.1|\n",
            "| 30.50410754091404|32.9|\n",
            "| 36.71084264945591|42.3|\n",
            "|30.375442767948094|34.7|\n",
            "| 19.59899004373794|18.5|\n",
            "|34.091019647336616|34.9|\n",
            "|30.499298725303746|23.5|\n",
            "|31.920010397360237|27.9|\n",
            "|26.093385173968073|24.8|\n",
            "| 21.68551219244037|20.7|\n",
            "|27.359893315366023|23.2|\n",
            "| 28.33678346582046|28.0|\n",
            "| 30.92596058671456|24.8|\n",
            "|27.029279789450797|22.6|\n",
            "|22.283609778223155|22.5|\n",
            "|23.315067240745766|22.4|\n",
            "| 30.18759562782573|30.5|\n",
            "| 22.68581977785566|20.3|\n",
            "+------------------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE): 4.671806485171285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KMeans model"
      ],
      "metadata": {
        "id": "mCeHgc4hdx5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "# Create a KMeans model\n",
        "kmeans = KMeans(k=5, seed=42)\n",
        "# Train the model\n",
        "model = kmeans.fit(data_features)\n",
        "# Make predictions on the data\n",
        "predictions = model.transform(data_features)\n",
        "# Display the cluster assignment for each data point\n",
        "predictions.select(\"features\", \"prediction\").show()\n",
        "# Evaluate the clustering model (if you have ground truth labels)\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette_score = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette Score:\", silhouette_score)"
      ],
      "metadata": {
        "id": "rWhdRiWrAxNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3db65f-2559-4dcf-ce6e-8ac9ed644bf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|            features|prediction|\n",
            "+--------------------+----------+\n",
            "|[0.00632,18.0,2.3...|         4|\n",
            "|[0.02731,0.0,7.07...|         2|\n",
            "|[0.02729,0.0,7.07...|         2|\n",
            "|[0.03237,0.0,2.18...|         2|\n",
            "|[0.06905,0.0,2.18...|         2|\n",
            "|[0.02985,0.0,2.18...|         2|\n",
            "|[0.08829,12.5,7.8...|         4|\n",
            "|[0.14455,12.5,7.8...|         4|\n",
            "|[0.21124,12.5,7.8...|         4|\n",
            "|[0.17004,12.5,7.8...|         4|\n",
            "|[0.22489,12.5,7.8...|         4|\n",
            "|[0.11747,12.5,7.8...|         4|\n",
            "|[0.09378,12.5,7.8...|         4|\n",
            "|[0.62976,0.0,8.14...|         4|\n",
            "|[0.63796,0.0,8.14...|         4|\n",
            "|[0.62739,0.0,8.14...|         4|\n",
            "|[1.05393,0.0,8.14...|         4|\n",
            "|[0.7842,0.0,8.14,...|         4|\n",
            "|[0.80271,0.0,8.14...|         4|\n",
            "|[0.7258,0.0,8.14,...|         4|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Silhouette Score: 0.6354883093822121\n"
          ]
        }
      ]
    }
  ]
}