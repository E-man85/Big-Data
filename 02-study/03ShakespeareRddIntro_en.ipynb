{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lsmn7uCkEJoHI7uAEuDrPz2z3lWoupTM","timestamp":1690372018717},{"file_id":"1T6W0fZnvQER6kBMTKxBA1M9M-pbR7_cg","timestamp":1684580233750}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Academic exercise for study\n","\n","***Summary***\n","\n","This notebook introduces you to the use of Resilient Distributed Datasets (RDDs) in Spark. RDDs are the foundation of Spark's \"low-level\" API.\n","Though we will mostly use the higher-level DataFrame API, RDDs are more convenient to introduce a number of core concepts in Spark and to express MapReduce-like computations.\n","\n","__References__:\n","  - [RDD API documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)."],"metadata":{"id":"0JicEjva939p"}},{"cell_type":"markdown","source":["## Install pyspark and customize Colab configuration\n","\n","We need to install and setup pyspark first because it is not installed by default in the Colab runtime. The pyspark installation will persist until the runtime is recycled."],"metadata":{"id":"3b371Yi_-AhK"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iImyevm7I4I","executionInfo":{"status":"ok","timestamp":1690372350670,"user_tz":-60,"elapsed":71683,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"a2ee3b4b-683b-46ad-9240-86a0bcc74f8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Python interface to Spark\n","!pip install pyspark --quiet\n","# Installation and update of the PyDrive library, for interacting with Google Drive using Python.\n","!pip install -U -q PyDrive --quiet\n","# Install OpenJDK 8\n","!apt install openjdk-8-jdk-headless &> /dev/null\n","# Download the ngrok zip file to access the local server over the internet\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n","# Unzip the ngrok zip file\n","!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n","# Starts ngrok, allowing HTTP traffic on port 4050\n","get_ipython().system_raw('./ngrok http 4050 &')\n","# Import the Python os module\n","import os\n","# Sets the JAVA_HOME environment variable to the location of Java\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""]},{"cell_type":"markdown","source":["## Initialize Spark\n","\n","This is the typical bootstrap to get Spark working on  standalone mode. We obtain:\n","\n","- `spark`: a `SparkSession` object - this notebook acts as __driver program__ that communicates with Spark through the Spark session (we will talk about Spark's internal architecture in a future class).\n","\n","- `sc`: a `SparkContext` object that exposes Spark's low-level API."],"metadata":{"id":"1mCCjtcl-Ow6"}},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession\n","conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"shakespeare\").setMaster(\"local[2]\")\n","sc = SparkSession.builder.config(conf=conf).getOrCreate()"],"metadata":{"id":"EFl5K-5c7bIk","executionInfo":{"status":"ok","timestamp":1690372378026,"user_tz":-60,"elapsed":7364,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Load data\n"],"metadata":{"id":"y9FVWf_LCsj0"}},{"cell_type":"code","source":["# Download from http to local file\n","!wget --quiet --show-progress https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W01yTlEc7n35","executionInfo":{"status":"ok","timestamp":1690372383981,"user_tz":-60,"elapsed":315,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"62073f13-d64b-48e9-f726-6689233044c2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\rt8.shakespeare.txt    0%[                    ]       0  --.-KB/s               \rt8.shakespeare.txt  100%[===================>]   5.21M  --.-KB/s    in 0.02s   \n"]}]},{"cell_type":"markdown","source":["## What is an RDD?\n","\n","RDDs are \"immutable, partitioned collection of elements that can be operated on in parallel\". This means:\n","\n","- They are immutable, because you cannot change the contents of RDDs after creating them.\n","\n","- Their data is split in several partitions.\n","\n","- The partitions can be processed in parallel.\n","\n","RDD processing involves [__transformations__](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) and [__actions__](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions):\n","\n","- __Transformations__:  create a new RDD from from a data source or a previously existing RDD. A directed acyclic graph (DAG) of transformations (in many cases taking form just as a sequence) that defines an RDD is called the RDD __lineage__.\n","\n","- Transformations are evaluated __lazily__, meaning that they will execute only when triggered by __actions__ that\n","return a value to the driver program."],"metadata":{"id":"cYRIBGLf9zBB"}},{"cell_type":"markdown","source":["## The word count program\n","\n","The `wordCount` function definition and usage below is an example of RDD processing:\n","\n","- The RDD is specified in `wordCount` by a chain of transformations:  `textFile` (defining the data source), `map`, `flatMap`, `reduceByKey`, and `sortBy`.\n","- After the call to `wordCount`, `collect` is an action that yields back the RDD contents as a list (of key-value pairs)."],"metadata":{"id":"amKkyqfy-4Af"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import string\n","\n","def wordCount(file):\n","    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n","    sc = spark.sparkContext\n","# Define the punctuation filter\n","    punctFilter = str.maketrans(string.punctuation + '0123456789', ' ' * (len(string.punctuation) + 10))\n","    '''\n","# example\n","import string\n","punctFilter = str.maketrans(string.punctuation + '0123456789', ' ' * (len(string.punctuation) + 10))\n","text = \"Hello! This is a sentence with some numbers 123 and punctuation!!!\"\n","filtered_text = text.translate(punctFilter)\n","print(filter_text)\n","    Hello This is a sentence with some numbers and punctuation\n","In this example, the punctFilter translation table is used in the translate() method to replace all punctuation characters\n","and digits for spaces in the text variable.\n","The result is the string filtered_text, where punctuation characters and digits have been replaced with spaces.\n","    '''\n","# Load the text file into an RDD\n","    rdd = sc.textFile(file)\n","# Perform word count\n","    word_counts = rdd.flatMap(lambda line: line.translate(punctFilter).strip().upper().split()) \\\n","                    .map(lambda word: (word, 1)) \\\n","                    .reduceByKey(lambda x, y: x + y) \\\n","                    .sortBy(lambda kv: kv[1], ascending=False)\n","    return word_counts\n","'''\n","rdd.flatMap(lambda line: line.translate(punctFilter).strip().upper().split()):\n","The RDD rdd is flattened to generate a new RDD, where each line is translated using the punctFilter table to replace punctuation characters and digits with spaces.\n","Then a strip() is done to remove whitespace at the beginning and end of the line,\n","upper() to convert all words to uppercase,\n","and split() to split the line into separate words.\n","This results in an RDD of individual words.\n","\n",".map(lambda word: (word, 1)):\n","A mapping operation is applied to each word in the RDD.\n","Each word maps to a tuple (word, 1), where word is the word itself and 1 is an initial count value.\n","\n",".reduceByKey(lambda x, y: x + y): The tuples are reduced by key, adding the corresponding values.\n","This effectively performs word counting by aggregating the occurrence count of each word\n","\n",".sortBy(lambda kv: kv[1], ascending=False):\n","The RDD is sorted based on the count value (second element of the tuple) in descending order.\n","This results in an RDD ordered by word frequency, highest to lowest.\n","'''\n","# Path to the Shakespeare text file\n","data_file = '/content/t8.shakespeare.txt'\n","# Perform word count\n","words_rdd = wordCount(data_file)\n","# Collect and print the results\n","data = words_rdd.collect()\n","# Display the first 20 elements in the list\n","print('First 20 elements in the list with %d elements:' % len(data))\n","print(data[:20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewQ1lYDK8KaL","executionInfo":{"status":"ok","timestamp":1690372787327,"user_tz":-60,"elapsed":13278,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"d70d8a82-0d44-4456-f666-66cf6bbd2636"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["First 20 elements in the list with 23683 elements:\n","[('THE', 27660), ('AND', 26784), ('I', 22538), ('TO', 19819), ('OF', 18191), ('A', 14746), ('YOU', 13860), ('MY', 12489), ('THAT', 11549), ('IN', 11123), ('IS', 9784), ('D', 8960), ('NOT', 8740), ('FOR', 8341), ('WITH', 8016), ('ME', 7777), ('IT', 7737), ('S', 7723), ('BE', 7130), ('YOUR', 6885)]\n"]}]},{"cell_type":"code","source":["# view 20 elements with indexing\n","data[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZVb7j5qAXGD","executionInfo":{"status":"ok","timestamp":1690372803758,"user_tz":-60,"elapsed":256,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"5dcdf810-c614-4123-fd67-0e03a2104c65"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('THE', 27660),\n"," ('AND', 26784),\n"," ('I', 22538),\n"," ('TO', 19819),\n"," ('OF', 18191),\n"," ('A', 14746),\n"," ('YOU', 13860),\n"," ('MY', 12489),\n"," ('THAT', 11549),\n"," ('IN', 11123),\n"," ('IS', 9784),\n"," ('D', 8960),\n"," ('NOT', 8740),\n"," ('FOR', 8341),\n"," ('WITH', 8016),\n"," ('ME', 7777),\n"," ('IT', 7737),\n"," ('S', 7723),\n"," ('BE', 7130),\n"," ('YOUR', 6885)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Derive relative word frequencies\n","\n","We now use `words_rdd` and `total_words` to define an RDD to hold relative word frequencies rather than absolute frequences. That is: for  `(word,n)` in `words_rdd` we obtain `(word, n / total_words)` in the new RDD."],"metadata":{"id":"98UYTZUi-8-E"}},{"cell_type":"code","source":["# Reduce is another action (not a transformation like reduceByKey!)\n","'''\n","words_rdd.values(): The values() function is called on the words_rdd RDD to get just the values ​​(word counts) from the RDD.\n","This returns a new RDD containing just the word counts, without the actual words.\n",".reduce(lambda x, y: x + y): The reduce() function is applied to the word count RDD to reduce it to a single value.\n","The lambda lambda x, y: x + y specifies the reduction operation, which in this case is the simple sum of counts.\n","The reduce() function repeatedly combines pairs of values ​​until only one value remains, which is the sum total of the word counts.\n","At the end, the total_words variable will contain the total sum of all word counts present in the RDD words_rdd\n","'''\n","# We can use it obtain the total number of words\n","total_words = words_rdd.values().reduce(lambda x,y: x+y)\n","print('Total words [1]: %d' % total_words)\n","# We can just use the sum utility action in this case\n","total_words = words_rdd.values().sum()\n","print('Total words [2]: %d' % total_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rT8i40qg9SPv","executionInfo":{"status":"ok","timestamp":1690373073427,"user_tz":-60,"elapsed":1136,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"3979460c-5541-48c9-c526-703482906e54"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words [1]: 928012\n","Total words [2]: 928012\n"]}]},{"cell_type":"markdown","source":["##  Compare relative frequencies with Zipf's law\n","\n","Now derive an RDD with tuples of the form `(word,(freq,zipfEstimate))`.\n","\n","You can start by mapping `(word,freq)`\n","to `((word,freq), index)` using the `zipWithIndex()` transformation as shown below.\n","\n","Then we use the index to map `((word,freq),index)` onto `(word,(freq,zipf_law(index))`.\n"],"metadata":{"id":"wUEUoMrZ_Gmp"}},{"cell_type":"code","source":["freq_rdd = words_rdd.map(lambda kv: (kv[0], kv[1] / total_words))\n","freq_rdd.take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6zq2-2v9WF2","executionInfo":{"status":"ok","timestamp":1690373229795,"user_tz":-60,"elapsed":302,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"ff7fc22f-a792-4cf1-ece9-7b6221890595"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('THE', 0.029805649064882783),\n"," ('AND', 0.028861695753934217),\n"," ('I', 0.024286323883742883),\n"," ('TO', 0.021356404874074905),\n"," ('OF', 0.019602117214001544),\n"," ('A', 0.015889880734300848),\n"," ('YOU', 0.01493515170062456),\n"," ('MY', 0.01345780011465369),\n"," ('THAT', 0.012444882178247695),\n"," ('IN', 0.01198583639004668)]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Simpler alternative\n","freq_rdd = words_rdd.mapValues(lambda count: count / total_words)\n","freq_rdd.take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8QKE_Z-e9YQ9","executionInfo":{"status":"ok","timestamp":1690373322774,"user_tz":-60,"elapsed":616,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"cf0dae38-cf18-42d7-c6ab-ccae9882c7b7"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('THE', 0.029805649064882783),\n"," ('AND', 0.028861695753934217),\n"," ('I', 0.024286323883742883),\n"," ('TO', 0.021356404874074905),\n"," ('OF', 0.019602117214001544),\n"," ('A', 0.015889880734300848),\n"," ('YOU', 0.01493515170062456),\n"," ('MY', 0.01345780011465369),\n"," ('THAT', 0.012444882178247695),\n"," ('IN', 0.01198583639004668)]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# (word,freq) -> ((word,freq),index)\n","'''\n","the resulting RDD rdd_with_index will contain elements composed of a tuple containing the original RDD element freq_rdd and the index corresponding to that element.\n","This new RDD can be useful in scenarios where it is necessary to index or identify RDD elements sequentially.\n","'''\n","rdd_with_index = freq_rdd.zipWithIndex()\n","rdd_with_index.take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2bUu0uU9a_p","executionInfo":{"status":"ok","timestamp":1690373389545,"user_tz":-60,"elapsed":1137,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"a4b23849-9aea-487a-c2b2-ef8801e556c4"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('THE', 0.029805649064882783), 0),\n"," (('AND', 0.028861695753934217), 1),\n"," (('I', 0.024286323883742883), 2),\n"," (('TO', 0.021356404874074905), 3),\n"," (('OF', 0.019602117214001544), 4),\n"," (('A', 0.015889880734300848), 5),\n"," (('YOU', 0.01493515170062456), 6),\n"," (('MY', 0.01345780011465369), 7),\n"," (('THAT', 0.012444882178247695), 8),\n"," (('IN', 0.01198583639004668), 9)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Zipf's law - simpler approximation\n","def zipf_law(i):\n","  return 0.1 / (i+1)\n","# TODO ((word,freq),index) -> (word, (freq,zipf_law(index)))\n","rdd_cmp_zipf = rdd_with_index.\\\n","               map(lambda x: (x[0][0], (x[0][1], zipf_law(x[1]))))\n","'''\n","A new RDD called rdd_cmp_zipf is created from the rdd_with_index RDD.\n","The zipf_law(i) function is applied to each element of the RDD rdd_with_index to calculate the expected frequency according to Zipf's Law.\n","Here is an explanation of what each piece of code does:\n","zipf_law(i): It is a defined function that takes a parameter i representing the index and calculates the expected frequency according to Zipf's Law.\n","In this case, the formula used is 0.1 / (i + 1). This function returns the expected frequency for a given index.\n","rdd_with_index.map(lambda x: (x[0][0], (x[0][1], zipf_law(x[1])))): The map() function is applied in the rdd_with_index RDD to map each element .\n","The lambda lambda x: (x[0][0], (x[0][1], zipf_law(x[1]))) is used to transform each element of the RDD into a tuple,\n","where the first element is the original word (x[0][0]), the second element is a tuple containing the original count and the\n","expected frequency according to Zipf's Law ((x[0][1], zipf_law(x[1]))).\n","In this way, the resulting RDD rdd_cmp_zipf will have elements composed of a tuple containing the original word,\n","the original count and expected frequency according to Zipf's Law for the index corresponding to that word.\n","This RDD can be used to compare the actual count with the expected frequency according to the Zipf distribution.\n","\n","'''\n","\n","rdd_cmp_zipf.take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTlQxRla9dD5","executionInfo":{"status":"ok","timestamp":1690373449551,"user_tz":-60,"elapsed":705,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"37376a94-421d-4433-959e-2eb7d4d18502"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('THE', (0.029805649064882783, 0.1)),\n"," ('AND', (0.028861695753934217, 0.05)),\n"," ('I', (0.024286323883742883, 0.03333333333333333)),\n"," ('TO', (0.021356404874074905, 0.025)),\n"," ('OF', (0.019602117214001544, 0.02)),\n"," ('A', (0.015889880734300848, 0.016666666666666666)),\n"," ('YOU', (0.01493515170062456, 0.014285714285714287)),\n"," ('MY', (0.01345780011465369, 0.0125)),\n"," ('THAT', (0.012444882178247695, 0.011111111111111112)),\n"," ('IN', (0.01198583639004668, 0.01))]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Suppose we want the maximum absolute error between actual frequencies and the Zipf's estimate\n","# (it's easy to guess that it will be the difference for the word \"THE\")\n","max_error = rdd_cmp_zipf\\\n","          .values()\\\n","          .map(lambda x: abs(x[0]-x[1]))\\\n","          .max()\n","max_error"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2T8BVUW9fr1","executionInfo":{"status":"ok","timestamp":1690373527041,"user_tz":-60,"elapsed":563,"user":{"displayName":"Emanuel Gomes","userId":"11589531906273730047"}},"outputId":"e597aed6-ac08-40ee-fb4c-9d9cdd76677c"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07019435093511722"]},"metadata":{},"execution_count":16}]}]}