{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "residential-checkout",
      "metadata": {
        "id": "residential-checkout"
      },
      "source": [
        "# Lazy RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sporting-williams",
      "metadata": {
        "id": "sporting-williams"
      },
      "source": [
        "## Academic exercise for study"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forced-georgia",
      "metadata": {
        "id": "forced-georgia"
      },
      "source": [
        "Let's see about how Spark achieves efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "animated-appreciation",
      "metadata": {
        "id": "animated-appreciation"
      },
      "source": [
        "### Install pyspark and customize Colab configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python interface to Spark\n",
        "!pip install pyspark --quiet\n",
        "# Installation and update of the PyDrive library, for interacting with Google Drive using Python.\n",
        "!pip install -U -q PyDrive --quiet\n",
        "# Install OpenJDK 8\n",
        "!apt install openjdk-8-jdk-headless &> /dev/null\n",
        "# Download the ngrok zip file to access the local server over the internet\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Unzip the ngrok zip file\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "# Starts ngrok, allowing HTTP traffic on port 4050\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "# Import the Python os module\n",
        "import os\n",
        "# Sets the JAVA_HOME environment variable to the location of Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQxtYGwncLcC",
        "outputId": "55ce632d-4ad5-4bd0-e8bb-25bd4c3904bb"
      },
      "id": "ZQxtYGwncLcC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Spark"
      ],
      "metadata": {
        "id": "4wPDHrGScQ3A"
      },
      "id": "4wPDHrGScQ3A"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "increasing-preliminary",
      "metadata": {
        "id": "increasing-preliminary"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[2]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "logical-catholic",
      "metadata": {
        "id": "logical-catholic"
      },
      "source": [
        "Now  let's again create an RDD from our movie records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "charitable-assets",
      "metadata": {
        "id": "charitable-assets"
      },
      "outputs": [],
      "source": [
        "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "optimum-terror",
      "metadata": {
        "id": "optimum-terror",
        "outputId": "fbec982c-78bd-4bd7-d0a0-1a70a08616e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "movies_rdd = sc.parallelize(movies)\n",
        "movies_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entertaining-investor",
      "metadata": {
        "id": "entertaining-investor"
      },
      "source": [
        "And then let's capitalize the movies, and select the movies that begin with `d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "frozen-retail",
      "metadata": {
        "id": "frozen-retail",
        "outputId": "25308fab-53b0-420e-8ef7-49782dd8b271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knowing-bleeding",
      "metadata": {
        "id": "knowing-bleeding"
      },
      "source": [
        "Now as we know, Spark will partition the dataset across the cores of the executors, and then map through the records in parallel, returning all of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-processor",
      "metadata": {
        "id": "accredited-processor"
      },
      "source": [
        "Now let's change the function so that this time, instead of returning all of the results, we just return the first result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "floating-louis",
      "metadata": {
        "id": "floating-louis",
        "outputId": "8ced8234-7734-4b88-acbf-a2187e8df036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "expired-leave",
      "metadata": {
        "id": "expired-leave"
      },
      "source": [
        "Now if we think about, this previous step, here we would not have to map through all of the steps just to return a single result.  And it turns out if we look at Spark, we can see that even though the dataset was distributed -- it only needed to perform work on a single partition to return one result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "herbal-future",
      "metadata": {
        "id": "herbal-future"
      },
      "source": [
        "This ability, to see the end result that needs to be returned, and to work efficiently to only take the needed steps to return those results, is a valuable feature when working with large datasets.  And we can better see how Spark accomplishes it in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitting-publisher",
      "metadata": {
        "id": "fitting-publisher"
      },
      "source": [
        "### A little experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electronic-engagement",
      "metadata": {
        "id": "electronic-engagement"
      },
      "source": [
        "If we run the code below, notice that nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "powered-bikini",
      "metadata": {
        "id": "powered-bikini",
        "outputId": "f71f1e69-7003-4c8f-b484-30f84f194461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[3] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-surprise",
      "metadata": {
        "id": "published-surprise"
      },
      "source": [
        "And even if we chain the map and the filter methods, still nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "exceptional-saturn",
      "metadata": {
        "id": "exceptional-saturn",
        "outputId": "d9251510-b081-47da-bd89-ddd5f5876640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[4] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'd')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fossil-toner",
      "metadata": {
        "id": "fossil-toner"
      },
      "source": [
        "It's only when we add a collect function on the end, will some data be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "variable-print",
      "metadata": {
        "id": "variable-print",
        "outputId": "d1d6558e-a908-4aeb-cd7a-dcd42a654de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "movies_rdd.filter(lambda movie: movie[0] == 'd').map(lambda movie: movie.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unlimited-civilian",
      "metadata": {
        "id": "unlimited-civilian"
      },
      "source": [
        "So above, nothing was returned when we ran the `map` and `collect` functions, because when we only executed those functions, Spark did not actually act on the data.  Then in the third line we finally did act on the data.  We told Spark that we want to both transform, and filter the data, and then return all of the results.  \n",
        "\n",
        "So it's only when we called the `collect` function that Spark's driver determined the tasks to then send off to the executors and return the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olive-format",
      "metadata": {
        "id": "olive-format"
      },
      "source": [
        "### Transformations and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "governmental-definition",
      "metadata": {
        "id": "governmental-definition"
      },
      "source": [
        "So above we can see that the functions `map` and `filter` do not actually perform any work on our data.  Instead steps are only kicked off when we call the `collect` method.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flush-remains",
      "metadata": {
        "id": "flush-remains"
      },
      "source": [
        "In Spark, the methods that kick off tasks and return results are called **actions** (eg. collect).  And methods like `map` and `transform` that do not are called **transformations**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noted-exposure",
      "metadata": {
        "id": "noted-exposure"
      },
      "source": [
        "1. Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protected-biology",
      "metadata": {
        "id": "protected-biology"
      },
      "source": [
        "So we already saw that transformations include `map` and `filter`, and our transformations do not actually return results to our users.  Here's a couple other transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imported-writer",
      "metadata": {
        "id": "imported-writer"
      },
      "source": [
        "* sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intense-swedish",
      "metadata": {
        "id": "intense-swedish"
      },
      "source": [
        "The `sample` method allows us to take a random sample from our dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "major-familiar",
      "metadata": {
        "id": "major-familiar",
        "outputId": "53057998-8ddc-4c8b-9a7c-cdbe3c00ae8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[6] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "movies_rdd.sample(fraction = .2, withReplacement = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "considered-copper",
      "metadata": {
        "id": "considered-copper"
      },
      "source": [
        "> Notice that it does not return any data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "supreme-idaho",
      "metadata": {
        "id": "supreme-idaho"
      },
      "source": [
        "* distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "handed-tennis",
      "metadata": {
        "id": "handed-tennis",
        "outputId": "e58f98e2-0489-4498-e423-31b90d4a5977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[11] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "movies_rdd.distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frequent-accounting",
      "metadata": {
        "id": "frequent-accounting"
      },
      "source": [
        "> Distinct finds the unique results.  Notice that it also does not return data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sustainable-tension",
      "metadata": {
        "id": "sustainable-tension"
      },
      "source": [
        "Finally, we have already seen `map`, which provides a one to one transformation of our records, and `select` which filters our data.  In each case, our transformations do not return data to us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "endless-columbus",
      "metadata": {
        "id": "endless-columbus"
      },
      "source": [
        "2. Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-parts",
      "metadata": {
        "id": "happy-parts"
      },
      "source": [
        "Actions are a bit more about the end result.  So far we've learned about `collect`, which returns *all* of the results of a series of transformations.  \n",
        "\n",
        "* Take\n",
        "\n",
        "We've also seen `take`, which limits our results to a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "radical-spotlight",
      "metadata": {
        "id": "radical-spotlight",
        "outputId": "782c4a37-c159-424a-db61-4341a04e04c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "movies_rdd.distinct().take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-diagram",
      "metadata": {
        "id": "hindu-diagram"
      },
      "source": [
        "> So `take` is similar to the `LIMIT` function in SQL. Notice that here our records are returned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "active-lending",
      "metadata": {
        "id": "active-lending"
      },
      "source": [
        "* Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "mathematical-advice",
      "metadata": {
        "id": "mathematical-advice",
        "outputId": "b5404f96-5ee3-4b16-ec45-206b4ba80957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "movies_rdd.distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "asian-hepatitis",
      "metadata": {
        "id": "asian-hepatitis"
      },
      "source": [
        "Count simply counts the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changing-orange",
      "metadata": {
        "id": "changing-orange"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iraqi-archive",
      "metadata": {
        "id": "iraqi-archive"
      },
      "source": [
        "So we can see that, our actions have a bit of finality to them.  To get a better sense of the transformation and action functions, it's worth looking at the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "special-driving",
      "metadata": {
        "id": "special-driving"
      },
      "source": [
        "### Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appointed-detroit",
      "metadata": {
        "id": "appointed-detroit"
      },
      "source": [
        "[Berkley White Paper](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
        "\n",
        "[Pyspark RDD Methods blog](https://www.nbshare.io/notebook/403283317/How-To-Analyze-Data-Using-Pyspark-RDD/)\n",
        "\n",
        "\n",
        "[Databricks Debugging Spark Streaming](https://docs.databricks.com/spark/latest/rdd-streaming/debugging-streaming-applications.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}